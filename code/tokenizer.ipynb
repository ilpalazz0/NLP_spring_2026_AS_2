{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c7b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "input_text_file = \"../data/Fuzuli-1.txt\"\n",
    "\n",
    "with open(input_text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    input_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44234871",
   "metadata": {},
   "source": [
    "# Whitespace Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace tokenization - letter only, stored in a dataframe\n",
    "tokens = re.findall(r\"\\p{L}+(?:-\\p{L}+)*\", input_text)\n",
    "df = pd.DataFrame(tokens, columns=[\"tokens\"])\n",
    "df['tokens'] = df['tokens'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f432ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique tokens with counts and percentage\n",
    "counts = df['tokens'].value_counts()\n",
    "total = counts.sum()\n",
    "unique_df = pd.DataFrame({\n",
    "    'token': counts.index,\n",
    "    'count': counts.values,\n",
    "    'frequency (%)': (counts.values/total*100).round(2)\n",
    "})\n",
    "\n",
    "# Total number of tokens\n",
    "print(\"Token number of tokens:\", total)\n",
    "\n",
    "# Number of unique tokens\n",
    "unique_count = df['tokens'].nunique() \n",
    "print(\"\\nNumber of unique tokens:\", unique_count)\n",
    "\n",
    "# Top 15 most frequent tokens\n",
    "print(\"\\n\",unique_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6719d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top15 = unique_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(top15['token'], top15['count'], color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Token\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 15 Most Frequent Tokens\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27c4a2",
   "metadata": {},
   "source": [
    "# Heaps' Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87784d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaps Law\n",
    "df_clean = df[~df['tokens'].isin(['.', '!', '?'])].copy()\n",
    "\n",
    "def get_N_V(df):\n",
    "    \n",
    "    N, V = [], []\n",
    "    all_tokens = df['tokens'].tolist()\n",
    "    seen = set()\n",
    "\n",
    "    for i, tok in enumerate(all_tokens, start=1):\n",
    "        N.append(i)\n",
    "        seen.add(tok)\n",
    "        V.append(len(seen))\n",
    "\n",
    "    return N, V\n",
    "\n",
    "# linear regression\n",
    "def lin_reg(N, V):\n",
    "\n",
    "    x = [math.log(n) for n in N]\n",
    "    y = [math.log(v) for v in V]\n",
    "\n",
    "    x_mean = sum(x) / len(x)\n",
    "    y_mean = sum(y) / len(y)\n",
    "\n",
    "    # y = a + beta*x | Least Square Estimation\n",
    "    beta = sum((xi - x_mean)*(yi - y_mean) for xi, yi in zip(x,y))/sum((xi - x_mean)**2 for xi in x)\n",
    "    a = y_mean - beta * x_mean\n",
    "    k = math.exp(a)\n",
    "\n",
    "    return k, beta\n",
    "\n",
    "N, V = get_N_V(df_clean)\n",
    "\n",
    "k, beta = lin_reg(N, V)\n",
    "\n",
    "print(\"Value of k:\", k, \"\\nValue of beta:\", beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Observed values\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(N, V, label=\"Observed Vocabulary Growth\", color=\"blue\")\n",
    "\n",
    "# Predicted values using Heaps' law\n",
    "V_pred = [k * (n ** beta) for n in N]\n",
    "plt.plot(N, V_pred, label=f\"Heaps' Law Fit (k={k:.2f}, β={beta:.2f})\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel(\"Total Number of Tokens (N)\")\n",
    "plt.ylabel(\"Vocabulary (V)\")\n",
    "plt.title(\"Vocabulary Growth vs. Total Tokens\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [list(token) for token in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aa16fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation(text):\n",
    "    # split on ., !, ? only when followed by space or end of string\n",
    "    sentence_endings = re.compile(\n",
    "        r'([.!?])([\\'\")\\]]*)(?=\\s|$)'  # punctuation + optional quotes/brackets, followed by space or end\n",
    "    )\n",
    "    # Delimiter for splitting\n",
    "    segmented = sentence_endings.sub(r'\\1\\2</s>', text)\n",
    "    sentences = [s.strip() for s in segmented.split('</s>') if s.strip()]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e6dda",
   "metadata": {},
   "source": [
    "# BPE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a28202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bpe(corpus, num_merges):\n",
    "    merges = []\n",
    "    merge_freqs = []   # store (pair, frequency) at each step\n",
    "    for _ in range(num_merges):\n",
    "        # Count pairs fresh each iteration\n",
    "        vocab = {}\n",
    "        for word in corpus:\n",
    "            for i in range(len(word)-1):\n",
    "                pair = (word[i], word[i+1])\n",
    "                vocab[pair] = vocab.get(pair, 0) + 1\n",
    "        \n",
    "        if not vocab:\n",
    "            break\n",
    "        \n",
    "        # Most frequent pair\n",
    "        most_frequent = max(vocab, key=vocab.get)\n",
    "        merges.append(most_frequent)\n",
    "        merge_freqs.append((most_frequent, vocab[most_frequent]))\n",
    "        \n",
    "        # Merge in corpus\n",
    "        new_symbol = ''.join(most_frequent)\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            i = 0\n",
    "            new_word = []\n",
    "            while i < len(word):\n",
    "                if i < len(word)-1 and (word[i], word[i+1]) == most_frequent:\n",
    "                    new_word.append(new_symbol)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_corpus.append(new_word)\n",
    "        corpus = new_corpus\n",
    "    \n",
    "    return merges, corpus, merge_freqs\n",
    "\n",
    "\n",
    "# Run BPE\n",
    "merges, updated_corpus, merge_freqs = learn_bpe(corpus, num_merges=1000)\n",
    "\n",
    "all_tokens = [tok for word in updated_corpus for tok in word]\n",
    "\n",
    "longest_token = max(all_tokens, key=len)\n",
    "\n",
    "print(\"Longest token:\", longest_token, \"with length:\", len(longest_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take top 15 by frequency\n",
    "top15 = sorted(merge_freqs, key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "# Print results\n",
    "print(\"Top 15 frequent BPE pairs:\")\n",
    "for pair, freq in top15:\n",
    "    print(f\"{pair}: {freq}\")\n",
    "\n",
    "# Plot histogram\n",
    "labels = [f\"{a}{b}\" for (a,b), _ in top15]\n",
    "counts = [freq for _, freq in top15]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(labels, counts, color=\"red\")\n",
    "plt.xlabel(\"BPE Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Top 15 Frequent BPE Tokens\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968816a1",
   "metadata": {},
   "source": [
    "# Processing Text With BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc98c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bpe(text, merges):\n",
    "    text = text.lower()\n",
    "    # Represent word as characters + end-of-word marker\n",
    "    chars = list(text) + [\"</w>\"]\n",
    "    \n",
    "    # Apply merges in the order they were learned\n",
    "    for merge in merges:\n",
    "        merged = ''.join(merge)\n",
    "        new_chars = []\n",
    "        i = 0\n",
    "        while i < len(chars):\n",
    "            if i < len(chars)-1 and (chars[i], chars[i+1]) == merge:\n",
    "                new_chars.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_chars.append(chars[i])\n",
    "                i += 1\n",
    "        chars = new_chars\n",
    "    \n",
    "    return ' '.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Bakıdan əlimə çatan qadın jurnallarının birində Kələkoş sözü diqqətimi cəlb etdi. \" \\\n",
    "\"Bu, elə reseptini axtardığım Kələkoş idi! Amma bu dəfı adın yanında resept də var idi! \" \\\n",
    "\"Beləliklə sirr dolu Kələkoşu bişirmək fürsətini əldə etmiş oldum. Çox ləzzətli şorbadır! \" \\\n",
    "\"Tez-tez menyumuza qolaq olur. Buyurun, siz de dadına baxın.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75e965",
   "metadata": {},
   "source": [
    "# Sample text - Sentence Segmentation + BPE tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb3e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(bpe_text):\n",
    "    return len(bpe_text.split())\n",
    "\n",
    "\n",
    "def sentence_tokenizer(sample_text):\n",
    "    segmented_sentences = sentence_segmentation(sample_text)\n",
    "\n",
    "    total_tokens = 0\n",
    "    i = 0\n",
    "\n",
    "    for sentence in segmented_sentences:\n",
    "        i+=1\n",
    "        res = apply_bpe(sentence, merges)\n",
    "        token_count = count_tokens(res)\n",
    "        total_tokens += token_count\n",
    "        print(f\"{i})\", res, f\"{token_count} tokens\", \"\\n\")\n",
    "    print(\"Total number of tokens:\", total_tokens)\n",
    "\n",
    "sentence_tokenizer(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d6b7af",
   "metadata": {},
   "source": [
    "# Input Sentence Segmentation + BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a623a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    sample_text = input(\"Enter a sentence (or press Enter to quit): \")\n",
    "    if not sample_text.strip():\n",
    "        break\n",
    "    sentence_tokenizer(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31586bab",
   "metadata": {},
   "source": [
    "# Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1032e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import LevensteinBK \n",
    "import unicodedata\n",
    "\n",
    "def strip_combining(text):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "with open(\"Fuzuli-I.txt\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    content = infile.read()\n",
    "\n",
    "content = content.lower()\n",
    "Tokens = re.findall(r\"\\b[^\\W\\d_]+(?:[\\'’\\-][^\\W\\d_]+)*\\b\", content)\n",
    "Tokens = [t.replace(\"’\", \"'\") for t in Tokens]\n",
    "Tokens = [strip_combining(t) for t in Tokens]\n",
    "vocabulary = set()\n",
    "\n",
    "for t in Tokens:\n",
    "    vocabulary.add(t)\n",
    "\n",
    "\n",
    "tt = LevensteinBK.BKtree(vocabulary)\n",
    "search_word = \"lə'li-nabın\"\n",
    "res = tt.search(search_word, 2)\n",
    "sorted_res = sorted(res, key = lambda w:( LevensteinBK.Levenstein(w.label, search_word)))\n",
    "\n",
    "for i in sorted_res:\n",
    "    print(i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
